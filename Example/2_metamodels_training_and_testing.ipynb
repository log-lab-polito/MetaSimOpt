{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AS/RS SIM OPT - Metamodel training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T13:16:40.420022Z",
     "start_time": "2025-05-26T13:16:38.301094Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize, suppress=True)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "from MetaSimOpt.metamodels import ModelFactory, RNN_Metamodel, LSTM_Metamodel, GRU_Metamodel\n",
    "from MetaSimOpt.handlers import HandlerTraining, HandlerMetamodel, HandlerHyperSearch\n",
    "from MetaSimOpt.utils import compute_residual_stats, print_residuals, plot_residuals, compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T13:16:41.703399Z",
     "start_time": "2025-05-26T13:16:40.429760Z"
    }
   },
   "outputs": [],
   "source": [
    "# set device and current directory\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(f'Device --> {device}')\n",
    "\n",
    "curr_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "dir_data = os.path.join(curr_dir,\"1_data\",\"processed\",\"metamodel_training\")\n",
    "\n",
    "path_features_rec = os.path.join(dir_data,\"features_rec.npy\")\n",
    "features_rec = np.load(path_features_rec)\n",
    "\n",
    "path_features_lin = os.path.join(dir_data,\"features_lin.npy\")\n",
    "features_lin = np.load(path_features_lin)\n",
    "\n",
    "path_output = os.path.join(dir_data,\"output.npy\")\n",
    "output_ann = np.load(path_output)\n",
    "\n",
    "print(f'Shape features rec = {features_rec.shape}')\n",
    "print(f'Shape features lin = {features_lin.shape}')\n",
    "\n",
    "# check if nan\n",
    "nan_indices = np.argwhere(np.isnan(output_ann))\n",
    "\n",
    "if nan_indices.size > 0:\n",
    "    print(\"The output contains NaN values at the following coordinates:\")\n",
    "    for index in nan_indices:\n",
    "        print(f\"Row: {index[0]}, Column: {index[1]}\")\n",
    "else:\n",
    "    print(\"The output does not contain NaN values.\")\n",
    "\n",
    "y_reg = output_ann[:,24:-1]\n",
    "y_reg = output_ann\n",
    "y_reg = np.mean(y_reg, axis=1, keepdims=True)\n",
    "print(f'Shape labels = {y_reg.shape}')\n",
    "\n",
    "data = {\n",
    "    'features_rec' : features_rec,\n",
    "    'features_lin' : features_lin,\n",
    "    'labels' : y_reg\n",
    "}\n",
    "\n",
    "input_size_rec = features_rec.shape[-1]\n",
    "input_size_lin = features_lin.shape[-1]\n",
    "output_size = y_reg.shape[-1]\n",
    "max_seq_length = features_rec.shape[1]\n",
    "\n",
    "x = [features_rec, features_lin]\n",
    "y = y_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_metamodels = {\n",
    "    \"rnn\" : RNN_Metamodel,\n",
    "    \"lstm\" : LSTM_Metamodel,\n",
    "    \"gru\" : GRU_Metamodel\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metamodel, metamodel_class in map_metamodels.items():\n",
    "    \n",
    "    print(f\"\\nSEARCHING HYPERPARAMETERS FOR REC CELL {metamodel.upper()}\")\n",
    "\n",
    "    factory = ModelFactory(model_class = metamodel_class, input_size_rec = input_size_rec, input_size_lin = input_size_lin, output_size = output_size, max_seq_length = max_seq_length)\n",
    "\n",
    "    tr_handler = HandlerTraining(\n",
    "        model_class = metamodel_class,\n",
    "        factory=factory,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    tr_handler.load_dataset(x = x, y = y, normalisation=\"min-max\")\n",
    "    tr_handler.set_loss_function(loss_function = nn.L1Loss)\n",
    "    search_space_model = tr_handler.model_class.get_search_space()\n",
    "    search_space_training = tr_handler.get_search_space(model_class = tr_handler.model_class)\n",
    "    search_space = search_space_model | search_space_training\n",
    "\n",
    "    # hyperparameter search\n",
    "\n",
    "    path_dir = os.path.join(curr_dir,\"2_training_and_testing_results\",\"hyperparameters_search\",f'{metamodel}')\n",
    "\n",
    "    search_method = \"random\"\n",
    "    validation_method = \"kfold\"\n",
    "    n_folds = 5\n",
    "    searcher = HandlerHyperSearch(handler = tr_handler, search_space = search_space, method = search_method, cv_mode = validation_method, n_folds = n_folds, path_dir = path_dir)\n",
    "    searcher.enable_pruner(method = \"median\", n_startup_trials = 10, n_warmup_steps = 50)\n",
    "\n",
    "    print(f'Search method: {search_method}, validation method: {validation_method}')\n",
    "    best_result = searcher.run(n_trials = 250, clear_results = False, reset_study = False)\n",
    "\n",
    "    print(\"\\nBest result\")\n",
    "    print(f'Loss -> {best_result[\"val_loss\"]}')\n",
    "    print(f'Hyperparameters model -> {best_result[\"hyper_model\"]}')\n",
    "    print(f'Hyperparameters training -> {best_result[\"hyper_train\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_metamodels = {\n",
    "    \"rnn\" : RNN_Metamodel,\n",
    "    \"lstm\" : LSTM_Metamodel,\n",
    "    \"gru\" : GRU_Metamodel\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_model_rnn = {}\n",
    "hyper_model_lstm = {}\n",
    "hyper_model_gru = {}\n",
    "\n",
    "hyper_model_rnn[\"hid_rec_layers\"] = 3\n",
    "hyper_model_rnn[\"hid_rec_size\"] = 8\n",
    "hyper_model_rnn[\"rec_dropout\"] = 0.0\n",
    "hyper_model_rnn[\"hid_lin_layers_1\"] = 1\n",
    "hyper_model_rnn[\"hid_lin_size_1_0\"] = 128\n",
    "hyper_model_rnn[\"hid_lin_size_1_1\"] = 0\n",
    "hyper_model_rnn[\"hid_lin_layers_2\"] = 1\n",
    "hyper_model_rnn[\"hid_lin_size_2_0\"] = 128\n",
    "hyper_model_rnn[\"hid_lin_size_2_1\"] = 0\n",
    "hyper_model_rnn[\"linear_dropout_1_0\"] = 0.3\n",
    "hyper_model_rnn[\"linear_dropout_1_1\"] = 0.0\n",
    "hyper_model_rnn[\"linear_dropout_2_0\"] = 0.3\n",
    "hyper_model_rnn[\"linear_dropout_2_1\"] = 0.0\n",
    "hyper_model_rnn[\"bidirectional\"] = True\n",
    "\n",
    "hyper_model_lstm[\"hid_rec_layers\"] = 1\n",
    "hyper_model_lstm[\"hid_rec_size\"] = 8\n",
    "hyper_model_lstm[\"rec_dropout\"] = 0.0\n",
    "hyper_model_lstm[\"hid_lin_layers_1\"] = 1\n",
    "hyper_model_lstm[\"hid_lin_size_1_0\"] = 32\n",
    "hyper_model_lstm[\"hid_lin_size_1_1\"] = 0\n",
    "hyper_model_lstm[\"hid_lin_layers_2\"] = 1\n",
    "hyper_model_lstm[\"hid_lin_size_2_0\"] = 64\n",
    "hyper_model_lstm[\"hid_lin_size_2_1\"] = 0\n",
    "hyper_model_lstm[\"linear_dropout_1_0\"] = 0.3\n",
    "hyper_model_lstm[\"linear_dropout_1_1\"] = 0.0\n",
    "hyper_model_lstm[\"linear_dropout_2_0\"] = 0.1\n",
    "hyper_model_lstm[\"linear_dropout_2_1\"] = 0.0\n",
    "hyper_model_lstm[\"bidirectional\"] = True\n",
    "\n",
    "hyper_model_gru[\"hid_rec_layers\"] = 1\n",
    "hyper_model_gru[\"hid_rec_size\"] = 8\n",
    "hyper_model_gru[\"rec_dropout\"] = 0.0\n",
    "hyper_model_gru[\"hid_lin_layers_1\"] = 1\n",
    "hyper_model_gru[\"hid_lin_size_1_0\"] = 32\n",
    "hyper_model_gru[\"hid_lin_size_1_1\"] = 0\n",
    "hyper_model_gru[\"hid_lin_layers_2\"] = 1\n",
    "hyper_model_gru[\"hid_lin_size_2_0\"] = 64\n",
    "hyper_model_gru[\"hid_lin_size_2_1\"] = 0\n",
    "hyper_model_gru[\"linear_dropout_1_0\"] = 0.3\n",
    "hyper_model_gru[\"linear_dropout_1_1\"] = 0.0\n",
    "hyper_model_gru[\"linear_dropout_2_0\"] = 0.1\n",
    "hyper_model_gru[\"linear_dropout_2_1\"] = 0.0\n",
    "hyper_model_gru[\"bidirectional\"] = True\n",
    "\n",
    "set_hyper_model = {\n",
    "    'rnn' : hyper_model_rnn,\n",
    "    'lstm' : hyper_model_lstm,\n",
    "    'gru' : hyper_model_gru\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set training hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_training_rnn = {}\n",
    "hyper_training_lstm = {}\n",
    "hyper_training_gru = {}\n",
    "\n",
    "hyper_training_rnn[\"epochs\"] = 400\n",
    "hyper_training_rnn[\"batch_size\"] = 32\n",
    "hyper_training_rnn[\"learning_rate\"] = 5e-4\n",
    "hyper_training_rnn[\"w_decay\"] = 1e-3\n",
    "hyper_training_rnn[\"l1_lambda\"] = 0\n",
    "\n",
    "hyper_training_lstm[\"epochs\"] = 400\n",
    "hyper_training_lstm[\"batch_size\"] = 32\n",
    "hyper_training_lstm[\"learning_rate\"] = 5e-4\n",
    "hyper_training_lstm[\"w_decay\"] = 1e-3\n",
    "hyper_training_lstm[\"l1_lambda\"] = 0\n",
    "\n",
    "hyper_training_gru[\"epochs\"] = 400\n",
    "hyper_training_gru[\"batch_size\"] = 32\n",
    "hyper_training_gru[\"learning_rate\"] = 5e-4\n",
    "hyper_training_gru[\"w_decay\"] = 1e-3\n",
    "hyper_training_gru[\"l1_lambda\"] = 0\n",
    "\n",
    "set_hyper_training = {\n",
    "    'rnn' : hyper_training_rnn,\n",
    "    'lstm' : hyper_training_lstm,\n",
    "    'gru' : hyper_training_gru\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and validate (k-fold) one model for each type of rec cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "\n",
    "for metamodel, metamodel_class in map_metamodels.items():\n",
    "\n",
    "    print(f\"\\nTRAINING AND VALIDATING METAMODEL WITH REC CELL {metamodel.upper()}\")\n",
    "\n",
    "    factory = ModelFactory(model_class = metamodel_class, input_size_rec = input_size_rec, input_size_lin = input_size_lin, output_size = output_size, max_seq_length = max_seq_length)\n",
    "\n",
    "    tr_handler = HandlerTraining(\n",
    "        model_class = metamodel_class,\n",
    "        factory = factory,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    tr_handler.load_dataset(x = x, y = y, normalisation = \"standard\")\n",
    "    tr_handler.set_model_hyperparameters(set_hyper_model[metamodel])\n",
    "    tr_handler.set_training_hyperparameters(set_hyper_training[metamodel])\n",
    "    tr_handler.set_loss_function(nn.L1Loss)\n",
    "    tr_handler.set_optimiser(optim.Adam)\n",
    "\n",
    "    losses, val_losses, scores, val_scores = tr_handler.train(validation = True, k_fold = True, parallel = True, print_progress = True, n_folds = n_folds)\n",
    "    \n",
    "    path_dir = os.path.join(curr_dir,\"2_training_and_testing_results\",\"metamodel_trained\",f'{metamodel}')\n",
    "    file_name = f\"res_training_kfold_{metamodel}.xlsx\"\n",
    "    tr_handler.save_to_excel(path_dir = path_dir, file_name = file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and validate (single split) one model for each type of rec cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metamodel, metamodel_class in map_metamodels.items():\n",
    "\n",
    "    print(f\"\\nTRAINING AND VALIDATING METAMODEL WITH REC CELL {metamodel.upper()}\")\n",
    "\n",
    "    factory = ModelFactory(model_class = metamodel_class, input_size_rec = input_size_rec, input_size_lin = input_size_lin, output_size = output_size, max_seq_length = max_seq_length)\n",
    "\n",
    "    tr_handler = HandlerTraining(\n",
    "        model_class = metamodel_class,\n",
    "        factory = factory,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    tr_handler.load_dataset(x = x, y = y, normalisation = \"min-max\")\n",
    "    tr_handler.set_model_hyperparameters(set_hyper_model[metamodel])\n",
    "    tr_handler.set_training_hyperparameters(set_hyper_training[metamodel])\n",
    "    tr_handler.set_loss_function(nn.L1Loss)\n",
    "    tr_handler.set_optimiser(optim.Adam)\n",
    "\n",
    "    losses, val_losses, scores, val_scores = tr_handler.train(validation = True, test_size = 0.2)\n",
    "    \n",
    "    path_dir = os.path.join(curr_dir,\"2_training_and_testing_results\",\"metamodel_trained\",f'{metamodel}')\n",
    "    file_name = f\"res_training_split_{metamodel}.xlsx\"\n",
    "    tr_handler.save_to_excel(path_dir = path_dir, file_name = file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train one model for each type of rec cell with all the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_metamodels = {\n",
    "    \"rnn\" : RNN_Metamodel,\n",
    "    \"lstm\" : LSTM_Metamodel,\n",
    "    \"gru\" : GRU_Metamodel\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_model_rnn = {}\n",
    "hyper_model_lstm = {}\n",
    "hyper_model_gru = {}\n",
    "\n",
    "hyper_model_rnn[\"hid_rec_layers\"] = 3\n",
    "hyper_model_rnn[\"hid_rec_size\"] = 8\n",
    "hyper_model_rnn[\"rec_dropout\"] = 0.0\n",
    "hyper_model_rnn[\"hid_lin_layers_1\"] = 1\n",
    "hyper_model_rnn[\"hid_lin_size_1_0\"] = 128\n",
    "hyper_model_rnn[\"hid_lin_size_1_1\"] = 0\n",
    "hyper_model_rnn[\"hid_lin_layers_2\"] = 1\n",
    "hyper_model_rnn[\"hid_lin_size_2_0\"] = 128\n",
    "hyper_model_rnn[\"hid_lin_size_2_1\"] = 0\n",
    "hyper_model_rnn[\"linear_dropout_1_0\"] = 0.3\n",
    "hyper_model_rnn[\"linear_dropout_1_1\"] = 0.0\n",
    "hyper_model_rnn[\"linear_dropout_2_0\"] = 0.3\n",
    "hyper_model_rnn[\"linear_dropout_2_1\"] = 0.0\n",
    "hyper_model_rnn[\"bidirectional\"] = True\n",
    "\n",
    "hyper_model_lstm[\"hid_rec_layers\"] = 1\n",
    "hyper_model_lstm[\"hid_rec_size\"] = 8\n",
    "hyper_model_lstm[\"rec_dropout\"] = 0.0\n",
    "hyper_model_lstm[\"hid_lin_layers_1\"] = 1\n",
    "hyper_model_lstm[\"hid_lin_size_1_0\"] = 32\n",
    "hyper_model_lstm[\"hid_lin_size_1_1\"] = 0\n",
    "hyper_model_lstm[\"hid_lin_layers_2\"] = 1\n",
    "hyper_model_lstm[\"hid_lin_size_2_0\"] = 64\n",
    "hyper_model_lstm[\"hid_lin_size_2_1\"] = 0\n",
    "hyper_model_lstm[\"linear_dropout_1_0\"] = 0.3\n",
    "hyper_model_lstm[\"linear_dropout_1_1\"] = 0.0\n",
    "hyper_model_lstm[\"linear_dropout_2_0\"] = 0.1\n",
    "hyper_model_lstm[\"linear_dropout_2_1\"] = 0.0\n",
    "hyper_model_lstm[\"bidirectional\"] = True\n",
    "\n",
    "hyper_model_gru[\"hid_rec_layers\"] = 1\n",
    "hyper_model_gru[\"hid_rec_size\"] = 8\n",
    "hyper_model_gru[\"rec_dropout\"] = 0.0\n",
    "hyper_model_gru[\"hid_lin_layers_1\"] = 1\n",
    "hyper_model_gru[\"hid_lin_size_1_0\"] = 32\n",
    "hyper_model_gru[\"hid_lin_size_1_1\"] = 0\n",
    "hyper_model_gru[\"hid_lin_layers_2\"] = 1\n",
    "hyper_model_gru[\"hid_lin_size_2_0\"] = 64\n",
    "hyper_model_gru[\"hid_lin_size_2_1\"] = 0\n",
    "hyper_model_gru[\"linear_dropout_1_0\"] = 0.3\n",
    "hyper_model_gru[\"linear_dropout_1_1\"] = 0.0\n",
    "hyper_model_gru[\"linear_dropout_2_0\"] = 0.1\n",
    "hyper_model_gru[\"linear_dropout_2_1\"] = 0.0\n",
    "hyper_model_gru[\"bidirectional\"] = True\n",
    "\n",
    "set_hyper_model = {\n",
    "    'rnn' : hyper_model_rnn,\n",
    "    'lstm' : hyper_model_lstm,\n",
    "    'gru' : hyper_model_gru\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_training_rnn = {}\n",
    "hyper_training_lstm = {}\n",
    "hyper_training_gru = {}\n",
    "\n",
    "hyper_training_rnn[\"epochs\"] = 400\n",
    "hyper_training_rnn[\"batch_size\"] = 32\n",
    "hyper_training_rnn[\"learning_rate\"] = 5e-4\n",
    "hyper_training_rnn[\"w_decay\"] = 1e-3\n",
    "hyper_training_rnn[\"l1_lambda\"] = 0\n",
    "\n",
    "hyper_training_lstm[\"epochs\"] = 400\n",
    "hyper_training_lstm[\"batch_size\"] = 32\n",
    "hyper_training_lstm[\"learning_rate\"] = 5e-4\n",
    "hyper_training_lstm[\"w_decay\"] = 1e-3\n",
    "hyper_training_lstm[\"l1_lambda\"] = 0\n",
    "\n",
    "hyper_training_gru[\"epochs\"] = 400\n",
    "hyper_training_gru[\"batch_size\"] = 32\n",
    "hyper_training_gru[\"learning_rate\"] = 5e-4\n",
    "hyper_training_gru[\"w_decay\"] = 1e-3\n",
    "hyper_training_gru[\"l1_lambda\"] = 0\n",
    "\n",
    "set_hyper_training = {\n",
    "    'rnn' : hyper_training_rnn,\n",
    "    'lstm' : hyper_training_lstm,\n",
    "    'gru' : hyper_training_gru\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metamodel, metamodel_class in map_metamodels.items():\n",
    "\n",
    "    print(f\"TRAINING METAMODEL WITH REC CELL {metamodel.upper()}\")\n",
    "\n",
    "    factory = ModelFactory(model_class = metamodel_class, input_size_rec = input_size_rec, input_size_lin = input_size_lin, output_size = output_size, max_seq_length = max_seq_length)\n",
    "\n",
    "    tr_handler = HandlerTraining(\n",
    "        model_class = metamodel_class,\n",
    "        factory = factory,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    tr_handler.load_dataset(x = x, y = y, normalisation = \"min-max\")\n",
    "    tr_handler.set_model_hyperparameters(set_hyper_model[metamodel])\n",
    "    tr_handler.set_training_hyperparameters(set_hyper_training[metamodel])\n",
    "    tr_handler.set_loss_function(nn.MSELoss)\n",
    "    tr_handler.set_optimiser(optim.AdamW)\n",
    "\n",
    "    losses, val_losses, scores, val_scores = tr_handler.train(validation = False, compute_score = False)\n",
    "    \n",
    "    path_dir = os.path.join(curr_dir,\"2_training_and_testing_results\",\"metamodel_trained\",f'{metamodel}')\n",
    "\n",
    "    file_name = f\"metamodel_{metamodel}.pth\"\n",
    "    tr_handler.save_model(path_dir = path_dir, file_name = file_name, save_data_scaler = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T13:19:49.362777Z",
     "start_time": "2025-05-26T13:19:49.246380Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "dir_data = os.path.join(curr_dir,\"1_data\",\"processed\",\"metamodel_testing\")\n",
    "\n",
    "path_features_rec = os.path.join(dir_data,\"features_rec.npy\")\n",
    "features_rec = np.load(path_features_rec)\n",
    "\n",
    "path_features_lin = os.path.join(dir_data,\"features_lin.npy\")\n",
    "features_lin = np.load(path_features_lin)\n",
    "\n",
    "path_output = os.path.join(dir_data,\"output.npy\")\n",
    "output_ann = np.load(path_output)\n",
    "\n",
    "print(f'Shape features rec = {features_rec.shape}')\n",
    "print(f'Shape features lin = {features_lin.shape}')\n",
    "\n",
    "# check if nan\n",
    "nan_indices = np.argwhere(np.isnan(output_ann))\n",
    "\n",
    "if nan_indices.size > 0:\n",
    "    print(\"The output contains NaN values at the following coordinates:\")\n",
    "    for index in nan_indices:\n",
    "        print(f\"Row: {index[0]}, Column: {index[1]}\")\n",
    "else:\n",
    "    print(\"The output does not contain NaN values.\")\n",
    "\n",
    "y_reg = output_ann[:,24:-1]\n",
    "y_reg = output_ann\n",
    "y_reg = np.mean(y_reg, axis=1, keepdims=True)\n",
    "y_reg = np.mean(y_reg, axis=1, keepdims=True)\n",
    "print(f'Shape labels = {y_reg.shape}')\n",
    "\n",
    "data = {\n",
    "    'features_rec' : features_rec,\n",
    "    'features_lin' : features_lin,\n",
    "    'labels' : y_reg\n",
    "}\n",
    "\n",
    "x = [features_rec, features_lin]\n",
    "y = y_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test metamodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T13:29:28.855256Z",
     "start_time": "2025-05-26T13:29:28.737360Z"
    }
   },
   "outputs": [],
   "source": [
    "map_metamodels = {\n",
    "    \"rnn\" : RNN_Metamodel,\n",
    "    \"lstm\" : LSTM_Metamodel,\n",
    "    \"gru\" : GRU_Metamodel\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results = False\n",
    "\n",
    "for metamodel, _ in map_metamodels.items():\n",
    "    \n",
    "    print(f\"\\nTESTING METAMODEL WITH REC CELL {metamodel.upper()}\")\n",
    "\n",
    "    mod_handler = HandlerMetamodel()\n",
    "    dir_metamodel = os.path.join(curr_dir, \"2_training_and_testing_results\",\"metamodel_trained\",f'{metamodel}')\n",
    "    file_metamodel = f'metamodel_{metamodel}.pth'\n",
    "    mod_handler.load_model_from_file(dir_metamodel = dir_metamodel, file_metamodel = file_metamodel)\n",
    "    mod_handler.load_data(data = x)\n",
    "    predictions = mod_handler.predict()\n",
    "\n",
    "    metrics = [\"mse\", \"mae\", \"mape\"]\n",
    "    results_metrics = compute_metrics(targets=y, predictions=predictions, metrics=metrics)\n",
    "    print(f\"MSE -> {round(results_metrics[0],2)}\")\n",
    "    print(f\"MAE -> {round(results_metrics[1],2)}\")\n",
    "    print(f\"MAPE -> {round(results_metrics[2],2)} %\")\n",
    "\n",
    "    residuals, residual_stats = compute_residual_stats(predictions=predictions, targets=y)\n",
    "    plot_residuals(residuals=residuals, predictions=predictions)\n",
    "    print_residuals(residual_stats)\n",
    "\n",
    "    if save_results: # save results\n",
    "        data = {\n",
    "            'targets': y_reg.flatten().squeeze(),\n",
    "            'predictions': predictions.flatten().squeeze(),\n",
    "            'mae' : results_metrics[0].flatten(),\n",
    "            'mape' : results_metrics[1].flatten()\n",
    "            }\n",
    "        df = pd.DataFrame(data)\n",
    "        dir_results = os.path.join(curr_dir, \"2_training_and_testing_results\",\"testing_results\",f'{metamodel}')\n",
    "        path_file = os.path.join(dir_results,f\"results_testing_{metamodel}.xlsx\")\n",
    "        df.to_excel(path_file, index=False)\n",
    "        print(f\"Results saved at {path_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results = False\n",
    "\n",
    "for metamodel, metamodel_class in map_metamodels.items():\n",
    "    \n",
    "    print(f\"\\nTESTING METAMODEL WITH REC CELL {metamodel.upper()}\")\n",
    "\n",
    "    dir_metamodel = os.path.join(curr_dir, \"2_training_and_testing_results\",\"metamodel_trained\",f'{metamodel}')\n",
    "    file_metamodel = f'metamodel_{metamodel}.pth'\n",
    "    mod_handler.load_model_from_file(dir_metamodel = dir_metamodel, file_metamodel = file_metamodel)\n",
    "    mod_handler.load_data(data = x)\n",
    "    predictions = mod_handler.predict(mc_samples = 10)\n",
    "\n",
    "    metrics = [\"mse\", \"mae\", \"mape\"]\n",
    "    results_metrics = compute_metrics(targets=y, predictions=predictions, metrics=metrics)\n",
    "    print(f\"MSE -> {round(results_metrics[0],2)}\")\n",
    "    print(f\"MAE -> {round(results_metrics[1],2)}\")\n",
    "    print(f\"MAPE -> {round(results_metrics[2],2)} %\")\n",
    "\n",
    "    residuals, residual_stats = compute_residual_stats(predictions=predictions, targets=y)\n",
    "    plot_residuals(residuals=residuals, predictions=predictions)\n",
    "    print_residuals(residual_stats)\n",
    "\n",
    "    if save_results: # save results\n",
    "        data = {\n",
    "            'targets': y_reg.flatten().squeeze(),\n",
    "            'predictions': predictions.flatten().squeeze(),\n",
    "            'mae' : results_metrics[0].flatten(),\n",
    "            'mape' : results_metrics[1].flatten()\n",
    "            }\n",
    "        df = pd.DataFrame(data)\n",
    "        dir_results = os.path.join(curr_dir, \"2_training_and_testing_results\",\"testing_results\",f'{metamodel}')\n",
    "        path_file = os.path.join(dir_results,f\"results_testing_{metamodel}.xlsx\")\n",
    "        df.to_excel(path_file, index=False)\n",
    "        print(f\"Results saved at {path_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metasimopt-DMTJvSgP-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
