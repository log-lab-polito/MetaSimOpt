{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AS/RS SIM OPT - Metamodel training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:24:01.413707Z",
     "start_time": "2025-05-26T09:24:01.408403Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "import os\n",
    "np.set_printoptions(suppress=True)\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "np.set_printoptions(suppress=True)\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:26:22.430008Z",
     "start_time": "2025-05-26T09:26:22.425081Z"
    }
   },
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:26:22.658993Z",
     "start_time": "2025-05-26T09:26:22.650423Z"
    }
   },
   "outputs": [],
   "source": [
    "phase = \"metamodel_training_and_testing\"\n",
    "file_name = \"1_data\" + os.sep + \"raw\" + os.sep + phase\n",
    "dir_raw_data = os.path.join(current_directory, file_name)\n",
    "iterations = os.listdir(dir_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:32:54.265312Z",
     "start_time": "2025-05-26T09:27:35.818796Z"
    }
   },
   "outputs": [],
   "source": [
    "list_json_order_set = []\n",
    "list_json_racks = []\n",
    "list_json_mls = []\n",
    "list_json_failures = []\n",
    "list_json_capacities = []\n",
    "list_output = []\n",
    "num_good_iteration = 0\n",
    "num_good_replications = 0\n",
    "total_num_simulation = len(iterations)\n",
    "max_replications = 1\n",
    "aggregation_factor = 100\n",
    "\n",
    "for i,iteration in enumerate(iterations):\n",
    "\n",
    "    path_iteration = os.path.join(dir_raw_data, iteration)\n",
    "\n",
    "    # Check if the iteration path is a directory\n",
    "\n",
    "    if os.path.isdir(path_iteration):\n",
    "\n",
    "        all_files = os.listdir(path_iteration)\n",
    "        print(f\"Iteration {i+1} / {total_num_simulation}\")\n",
    "        \n",
    "        #check context data\n",
    "        files_context = [f for f in all_files if os.path.isfile(os.path.join(path_iteration, f))]\n",
    "        dir_replications = [f for f in all_files if os.path.isdir(os.path.join(path_iteration, f))][0]\n",
    "\n",
    "        num_good_iteration += 1\n",
    "\n",
    "        for file_context in files_context:\n",
    "            iteration_capacity = None\n",
    "            path_file_context = os.path.join(path_iteration,file_context)\n",
    "            with open(path_file_context, \"r\") as file:\n",
    "                json_content = json.load(file)\n",
    "                if file_context == \"order_sets.json\":\n",
    "                    iteration_order_set = json_content\n",
    "                elif file_context == \"racks_data.json\":\n",
    "                    iteration_racks_data = json_content\n",
    "                elif file_context == \"mls_data.json\":\n",
    "                    iteration_mls = json_content\n",
    "                elif file_context == \"failure_data.json\":\n",
    "                    iteration_failures = json_content\n",
    "                elif file_context == \"capacity_data.json\":\n",
    "                    iteration_capacity = json_content\n",
    "\n",
    "        path_dir_replications = os.path.join(path_iteration,dir_replications)\n",
    "        files_replications = os.listdir(path_dir_replications)\n",
    "        num_replications = len(files_replications)\n",
    "        num_good_replications += num_replications\n",
    "        index_replication = 0\n",
    "\n",
    "        output_to_aggregate_all = []\n",
    "\n",
    "        for j in range(num_replications):\n",
    "\n",
    "            path_replication = os.path.join(path_dir_replications,files_replications[j])\n",
    "\n",
    "            with open(path_replication, \"r\") as file:\n",
    "                json_content = json.load(file)\n",
    "                if json_content[0][2] == True:\n",
    "                    output_value_all = [50000,1]\n",
    "                else:\n",
    "                    output_value_all = [json_content[0][1],0]\n",
    "\n",
    "                output_to_aggregate_all.append(output_value_all)\n",
    "\n",
    "        # sostituisco i valori quando accade deadlock con la media delle altre replicazioni\n",
    "        deadlock_data_all = [o[0] for o in output_to_aggregate_all if o[0] == 50000]\n",
    "        not_deadlock_data_all = [o[0] for o in output_to_aggregate_all if o[0] != 50000]\n",
    "        if deadlock_data_all:\n",
    "            mean_not_deadlock_all = np.mean(not_deadlock_data_all) # calcola una media generale just in case\n",
    "\n",
    "        while(num_replications):\n",
    "\n",
    "            output_to_aggregate = []\n",
    "\n",
    "            for _ in range(aggregation_factor):\n",
    "\n",
    "                if (num_replications):\n",
    "                    output_to_aggregate.append(output_to_aggregate_all[index_replication])\n",
    "\n",
    "                else: # esce se non bastano le replicazioni\n",
    "                    break\n",
    "\n",
    "                num_replications -= 1\n",
    "                index_replication += 1\n",
    "\n",
    "            # sostituisco i valori quando accade deadlock con la media delle altre replicazioni\n",
    "            deadlock_data = [o[0] for o in output_to_aggregate if o[0] == 50000]\n",
    "            not_deadlock_data = [o[0] for o in output_to_aggregate if o[0] != 50000]\n",
    "\n",
    "            if deadlock_data:\n",
    "\n",
    "                if not_deadlock_data:\n",
    "                    mean_not_deadlock = np.mean(not_deadlock_data) # se ci sono not_deadlock_data sostituisce con la media del cluster\n",
    "                else:\n",
    "                    mean_not_deadlock = mean_not_deadlock_all # se non ci sono not_deadlock_data sostituisce con la media generale\n",
    "\n",
    "                for o in output_to_aggregate:\n",
    "                    if o[0] == 50000:\n",
    "                        o[0] = mean_not_deadlock\n",
    "\n",
    "            aggreation = [np.mean(pair) for pair in list(zip(*output_to_aggregate))]\n",
    "            list_output.append(aggreation)\n",
    "            list_json_order_set.append(iteration_order_set)\n",
    "            list_json_racks.append(iteration_racks_data)\n",
    "            list_json_mls.append(iteration_mls)\n",
    "            list_json_failures.append(iteration_failures)\n",
    "            list_json_capacities.append(iteration_capacity if iteration_capacity is not None else 0)\n",
    "            \n",
    "print(\"Total iterations: \", total_num_simulation, \" Good iterations: \", num_good_iteration)\n",
    "print(\"Total replications: \", total_num_simulation*max_replications, \" Good replications: \", num_good_replications)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:37:52.488641Z",
     "start_time": "2025-05-26T09:35:34.103106Z"
    }
   },
   "outputs": [],
   "source": [
    "list_output = []\n",
    "num_good_iteration = 0\n",
    "num_good_replications = 0\n",
    "total_num_simulation = len(iterations)\n",
    "max_replications = 100\n",
    "aggregation_factor = 1\n",
    "\n",
    "for i, iteration in enumerate(iterations):\n",
    "\n",
    "    print(f\"Iteration {i+1} / {total_num_simulation}\")\n",
    "    \n",
    "    path_iteration = os.path.join(dir_raw_data, iteration)\n",
    "\n",
    "    # Check if the iteration path is a directory\n",
    "\n",
    "    if os.path.isdir(path_iteration):\n",
    "\n",
    "        all_files = os.listdir(path_iteration)\n",
    "        dir_replications = [f for f in all_files if os.path.isdir(os.path.join(path_iteration, f))][0]\n",
    "\n",
    "        path_dir_replications = os.path.join(path_iteration,dir_replications)\n",
    "        files_replications = os.listdir(path_dir_replications)\n",
    "        num_replications = len(files_replications)\n",
    "        num_good_replications += num_replications\n",
    "        index_replication = 0\n",
    "\n",
    "        output_all = []\n",
    "\n",
    "        for j in range(num_replications):\n",
    "\n",
    "            path_replication = os.path.join(path_dir_replications,files_replications[j])\n",
    "\n",
    "            with open(path_replication, \"r\") as file:\n",
    "                json_content = json.load(file)\n",
    "                if json_content[0][2] == True:\n",
    "                    output_value = 5000\n",
    "                else:\n",
    "                    output_value = json_content[0][1]\n",
    "\n",
    "                output_all.append(output_value)\n",
    "\n",
    "        output_all = sorted(output_all)\n",
    "        list_output.append(output_all)\n",
    "\n",
    "print(\"Total iterations: \", total_num_simulation, \" Good iterations: \", num_good_iteration)\n",
    "print(\"Total replications: \", total_num_simulation*max_replications, \" Good replications: \", num_good_replications)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features order sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:37:52.553961Z",
     "start_time": "2025-05-26T09:37:52.527410Z"
    }
   },
   "outputs": [],
   "source": [
    "list_item_classes = ['A', 'B', 'C']\n",
    "\n",
    "dict_class = {\n",
    "    100: 'A',\n",
    "    250: 'B',\n",
    "    500: 'C'\n",
    "}\n",
    "\n",
    "def get_item_class(item_id):\n",
    "    for num, item_class in dict_class.items():\n",
    "        if item_id <= num:\n",
    "            return item_class\n",
    "\n",
    "def feature_extraction_orders(order):\n",
    "    \n",
    "    # n_prod per class\n",
    "    # sum, min, avg, max, st_dev product_id per class\n",
    "    # sum, min, avg, max, st_dev quantity per class\n",
    "\n",
    "    ord_line_np = np.array(order)\n",
    "    item_ids = ord_line_np[:, 0]\n",
    "    item_quantities = ord_line_np[:, 1]\n",
    "    features = []\n",
    "\n",
    "    '''\n",
    "    # n_prod\n",
    "    features.append(len(order))\n",
    "\n",
    "    # sum, min, avg, max, st_dev quantity\n",
    "    features.extend([np.sum(item_quantities), np.min(item_quantities),\n",
    "                     np.mean(item_quantities), np.max(item_quantities), np.std(item_quantities)])\n",
    "\n",
    "    # sum, min, avg, max, st_dev product_id                      \n",
    "    features.extend([np.sum(item_ids), np.min(item_ids),\n",
    "                     np.mean(item_ids), np.max(item_ids), np.std(item_ids)])\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # n_prod per class\n",
    "    for item_class in list_item_classes:\n",
    "        class_mask = np.array([get_item_class(item_id) == item_class for item_id in item_ids])\n",
    "        features.append(np.sum(class_mask))\n",
    "\n",
    "    # sum, min, avg, max, st_dev product_id per class\n",
    "    for item_class in list_item_classes:\n",
    "        class_ids = item_ids[np.array([get_item_class(item_id) == item_class for item_id in item_ids])]\n",
    "        if not class_ids.size:\n",
    "            features.extend([0, 0, 0, 0, 0])\n",
    "        else:\n",
    "            features.extend([np.sum(class_ids), np.min(class_ids),\n",
    "                            np.mean(class_ids), np.max(class_ids), np.std(class_ids)])\n",
    "\n",
    "    # sum, min, avg, max, st_dev quantity per class\n",
    "    for item_class in list_item_classes:\n",
    "        class_quantities = item_quantities[np.array([get_item_class(item_id) == item_class for item_id in item_ids])]\n",
    "        if not class_quantities.size:\n",
    "            features.extend([0, 0, 0, 0, 0])\n",
    "        else:\n",
    "            features.extend([np.sum(class_quantities), np.min(class_quantities),\n",
    "                            np.mean(class_quantities), np.max(class_quantities), np.std(class_quantities)])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:38:49.112784Z",
     "start_time": "2025-05-26T09:37:52.626053Z"
    }
   },
   "outputs": [],
   "source": [
    "all_features_order_sets = []\n",
    "\n",
    "for i, ord_set in enumerate(list_json_order_set):\n",
    "\n",
    "    features_order_set = []\n",
    "\n",
    "    for j, order in enumerate(ord_set):\n",
    "\n",
    "        order_lines = [list(map(int, element.split(';'))) for element in order]\n",
    "        features_order = feature_extraction_orders(np.array(order_lines))\n",
    "        features_order_set.append(features_order)\n",
    "\n",
    "    all_features_order_sets.append(features_order_set)\n",
    "\n",
    "max_n_orders = max([len(order_set) for order_set in all_features_order_sets])\n",
    "for order_set in all_features_order_sets:\n",
    "    missing_order = max_n_orders - len(order_set)\n",
    "    for _ in range(missing_order):\n",
    "        order_set.extend([[0]*len(order_set[0])])\n",
    "\n",
    "np_features_order_sets = np.array(all_features_order_sets)\n",
    "print(np_features_order_sets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features racks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:38:52.515599Z",
     "start_time": "2025-05-26T09:38:52.493423Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features_rack(rack):\n",
    "    \n",
    "    features = []\n",
    "    sl_classes = ['A', 'B', 'C']\n",
    "\n",
    "    for sl_class in sl_classes:\n",
    "        \n",
    "        n_boxes = rack[sl_class][0]\n",
    "        deep_pos = rack[sl_class][1]\n",
    "        products_ids = rack[sl_class][2]\n",
    "        quantities = rack[sl_class][3]\n",
    "\n",
    "        # sum, min, avg, max, st dev for n_boxes, deep_pos, products_ids, and quantities\n",
    "        '''\n",
    "        feature_rack = [\n",
    "            np.sum(n_boxes), np.min(n_boxes), np.mean(n_boxes),\n",
    "            np.max(n_boxes), np.std(n_boxes),\n",
    "            np.sum(deep_pos), np.min(deep_pos), np.mean(deep_pos),\n",
    "            np.max(deep_pos), np.std(deep_pos),\n",
    "            np.sum(products_ids), np.min(products_ids), np.mean(products_ids),\n",
    "            np.max(products_ids), np.std(products_ids),\n",
    "            np.sum(quantities), np.min(quantities), np.mean(quantities),\n",
    "            np.max(quantities), np.std(quantities)\n",
    "        ]\n",
    "        '''\n",
    "\n",
    "        feature_rack = [\n",
    "            np.sum(n_boxes),np.sum(deep_pos),\n",
    "            np.sum(products_ids), np.min(products_ids), np.mean(products_ids),\n",
    "            np.max(products_ids), np.std(products_ids),\n",
    "            np.sum(quantities), np.min(quantities), np.mean(quantities),\n",
    "            np.max(quantities), np.std(quantities)\n",
    "        ]\n",
    "\n",
    "        features.extend(feature_rack)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:40:46.006450Z",
     "start_time": "2025-05-26T09:38:59.043625Z"
    }
   },
   "outputs": [],
   "source": [
    "all_features_rack = []\n",
    "\n",
    "for simulation in list_json_racks:\n",
    "    \n",
    "    features_rack = []\n",
    "\n",
    "    for rack in simulation:\n",
    "\n",
    "        loc_class = {\n",
    "            'A': [[], [], [], []],\n",
    "            'B': [[], [], [], []],\n",
    "            'C': [[], [], [], []]\n",
    "        }\n",
    "\n",
    "        for location in rack:\n",
    "            loc_class[location['sl_class']][0].extend([location['n_boxes']])\n",
    "            loc_class[location['sl_class']][1].extend([location['deep_occ']])\n",
    "            loc_class[location['sl_class']][2].extend(location['list_prod'])\n",
    "            loc_class[location['sl_class']][3].extend(location['list_qnt'])\n",
    "\n",
    "        features_rack.append(extract_features_rack(loc_class))\n",
    "\n",
    "    all_features_rack.append(features_rack)\n",
    "\n",
    "np_all_features_rack = np.array(all_features_rack)\n",
    "np_all_features_rack = np_all_features_rack.reshape(np_all_features_rack.shape[0],-1)\n",
    "print(np_all_features_rack.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features MLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:40:56.115051Z",
     "start_time": "2025-05-26T09:40:56.062306Z"
    }
   },
   "outputs": [],
   "source": [
    "all_features_mls = []\n",
    "\n",
    "for simulation in list_json_mls:\n",
    "    \n",
    "    features_mls = []\n",
    "\n",
    "    features_mls.extend([x for x in simulation[0]])\n",
    "    features_mls.extend([x for x in simulation[1]])\n",
    "    all_features_mls.append(features_mls)\n",
    "\n",
    "np_all_features_mls = np.array(all_features_mls)\n",
    "print(np_all_features_mls.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:41:00.457900Z",
     "start_time": "2025-05-26T09:41:00.372253Z"
    }
   },
   "outputs": [],
   "source": [
    "all_features_failures = []\n",
    "\n",
    "for simulation in list_json_failures:\n",
    "    features_failures = []\n",
    "    features_failures.extend(simulation)\n",
    "    all_features_failures.append(features_failures)\n",
    "\n",
    "np_all_features_failures = np.array(all_features_failures)\n",
    "print(np_all_features_failures.shape)\n",
    "\n",
    "bool_mask = np.logical_or(np_all_features_failures == True, np_all_features_failures == False)\n",
    "\n",
    "# Separate numerical and boolean variables\n",
    "np_all_features_numerical = np_all_features_failures[:, ~bool_mask.any(axis=0)]\n",
    "np_all_features_state = np_all_features_failures[:, bool_mask.any(axis=0)]\n",
    "\n",
    "print(\"Shape of Numerical Variables Array:\", np_all_features_numerical.shape)\n",
    "print(\"Shape of Boolean Variables Array:\", np_all_features_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:41:05.126383Z",
     "start_time": "2025-05-26T09:41:05.108114Z"
    }
   },
   "outputs": [],
   "source": [
    "np_all_features_capacities = np.array(list_json_capacities)\n",
    "print(np_all_features_capacities.shape)\n",
    "print(np_all_features_capacities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:41:09.193821Z",
     "start_time": "2025-05-26T09:41:09.166294Z"
    }
   },
   "outputs": [],
   "source": [
    "output_np = np.array(list_output)\n",
    "print(output_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:41:13.782397Z",
     "start_time": "2025-05-26T09:41:13.578245Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save(os.path.join(current_directory,\"1_data\",\"processed\",phase,\"features_order_sets.npy\"),np_features_order_sets)\n",
    "np.save(os.path.join(current_directory,\"1_data\",\"processed\",phase,\"features_racks.npy\"),np_all_features_rack)\n",
    "np.save(os.path.join(current_directory,\"1_data\",\"processed\",phase,\"features_mls.npy\"),np_all_features_mls)\n",
    "np.save(os.path.join(current_directory,\"1_data\",\"processed\",phase,\"features_failures.npy\"), np_all_features_numerical)\n",
    "np.save(os.path.join(current_directory,\"1_data\",\"processed\",phase,\"features_state.npy\"),np_all_features_state)\n",
    "np.save(os.path.join(current_directory,\"1_data\",\"processed\",phase,\"features_capacities.npy\"),np_all_features_capacities)\n",
    "np.save(os.path.join(current_directory,\"1_data\",\"processed\",phase,\"output.npy\"),output_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:41:18.883292Z",
     "start_time": "2025-05-26T09:41:18.865421Z"
    }
   },
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "directory = os.path.join(current_directory,\"1_data\",\"processed\",phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:41:24.259594Z",
     "start_time": "2025-05-26T09:41:24.194008Z"
    }
   },
   "outputs": [],
   "source": [
    "path_features_order_sets = os.path.join(directory,\"features_order_sets.npy\")\n",
    "features_order_sets_all = np.load(path_features_order_sets)\n",
    "\n",
    "path_features_racks = os.path.join(directory,\"features_racks.npy\")\n",
    "features_racks_all = np.load(path_features_racks)\n",
    "\n",
    "path_features_mls = os.path.join(directory,\"features_mls.npy\")\n",
    "features_mls_all = np.load(path_features_mls)\n",
    "\n",
    "path_features_failures = os.path.join(directory,\"features_failures.npy\")\n",
    "features_failures_all = np.load(path_features_failures)\n",
    "\n",
    "path_features_state = os.path.join(directory,\"features_state.npy\")\n",
    "features_state_all = np.load(path_features_state)\n",
    "\n",
    "path_output = os.path.join(directory,\"output.npy\")\n",
    "output_ann_all = np.load(path_output)\n",
    "\n",
    "np_multiplier = 1 - features_state_all\n",
    "features_upstate_all = features_failures_all * np_multiplier\n",
    "features_failures_all = features_failures_all * features_state_all\n",
    "\n",
    "print(np_multiplier[0,:])\n",
    "print(features_upstate_all[0,:])\n",
    "print(features_failures_all[0,:])\n",
    "\n",
    "print(f\"Order sets -> {features_order_sets_all.shape}\")\n",
    "print(f\"Racks -> {features_racks_all.shape}\")\n",
    "print(f\"MLS -> {features_mls_all.shape}\")\n",
    "print(f\"Upstates -> {features_upstate_all.shape}\")\n",
    "print(f\"Failures -> {features_failures_all.shape}\")\n",
    "print(f\"Output -> {output_ann_all.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:41:35.819395Z",
     "start_time": "2025-05-26T09:41:32.976842Z"
    }
   },
   "outputs": [],
   "source": [
    "palette = sns.color_palette('PuBu', as_cmap=True)\n",
    "\n",
    "new_slided_features_order_sets_all = np.reshape(features_order_sets_all, (features_order_sets_all.shape[0] * features_order_sets_all.shape[1], features_order_sets_all.shape[-1]))\n",
    "correlation_matrix, _ = spearmanr(new_slided_features_order_sets_all, axis=0)\n",
    "\n",
    "# Step 3: Plot the correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(correlation_matrix, cmap=palette, interpolation='nearest')\n",
    "plt.colorbar(label='Correlation Coefficient')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Features')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# features_index = [0,1,2,3,8,13,18,23,28] # n_prod per class, sum product_id per class, sum quantity per class\n",
    "features_index = [0,1,2,18,23,28] # n_prod per class, sum quantity per class\n",
    "# features_index = [18,23,28] # sum quantity per class\n",
    "slided_features_order_sets_all = features_order_sets_all[:,:,features_index]\n",
    "\n",
    "new_slided_features_order_sets_all = np.reshape(slided_features_order_sets_all, (slided_features_order_sets_all.shape[0] * slided_features_order_sets_all.shape[1], slided_features_order_sets_all.shape[-1]))\n",
    "\n",
    "# Step 2: Calculate correlation matrix using NumPy\n",
    "correlation_matrix = np.corrcoef(new_slided_features_order_sets_all[:,:], rowvar=False)\n",
    "correlation_matrix, _ = spearmanr(new_slided_features_order_sets_all[:,:], axis=0)\n",
    "print(correlation_matrix.shape)\n",
    "\n",
    "# Step 3: Plot the correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(correlation_matrix, cmap=palette, interpolation='nearest')\n",
    "plt.colorbar(label='Correlation Coefficient')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:41:46.105066Z",
     "start_time": "2025-05-26T09:41:45.548333Z"
    }
   },
   "outputs": [],
   "source": [
    "features_lin = np.concatenate((features_racks_all,features_mls_all,features_upstate_all,features_failures_all),axis=1)\n",
    "\n",
    "correlation_matrix, _ = spearmanr(features_lin, axis=0)\n",
    "\n",
    "# Step 3: Plot the correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(correlation_matrix, cmap=palette, interpolation='nearest')\n",
    "plt.colorbar(label='Correlation Coefficient')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:41:57.390814Z",
     "start_time": "2025-05-26T09:41:57.382694Z"
    }
   },
   "outputs": [],
   "source": [
    "# rack features\n",
    "\n",
    "#features_index = [0,2,7,12,14,19,24,26,31,36,38,43,48,50,55,60,62,67,72,74,79,84,86,91,96,98,103,108,110,115,120,122,127,132,134,139] # solo n_boxes per class, sum product_id per class, sum quantity per class\n",
    "features_index = [0,12,24,36,48,60,72,84,96,108,120,132] # solo n_boxes per class\n",
    "#features_index = [0,1,12,13,24,25,36,37,48,49,60,61,72,73,84,85,96,97,108,109,120,121,132,133] # n_boxes per class, n_deep_pos per class\n",
    "\n",
    "slided_features_racks_all = features_racks_all[:,features_index]\n",
    "print(slided_features_racks_all.shape)\n",
    "print(slided_features_racks_all[[0,],:])\n",
    "reshaped_array = slided_features_racks_all.reshape(slided_features_racks_all.shape[0], -1, 12)\n",
    "slided_features_racks_all = np.sum(slided_features_racks_all, axis=1)\n",
    "slided_features_racks_all = slided_features_racks_all[:, np.newaxis]\n",
    "\n",
    "print(slided_features_racks_all[[0]])\n",
    "print(slided_features_racks_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:44:04.790047Z",
     "start_time": "2025-05-26T09:44:04.731643Z"
    }
   },
   "outputs": [],
   "source": [
    "# upstate / failure features\n",
    "# new_failure = -1 * features_failures_all\n",
    "new_failure = features_failures_all\n",
    "new_state = features_upstate_all + new_failure\n",
    "print(new_state.shape)\n",
    "\n",
    "### conv aisles and mls\n",
    "\n",
    "list_features = list(range(20))\n",
    "features_aisles = new_state[:,list_features]\n",
    "\n",
    "list_mls = [0,5,10,15]\n",
    "list_conv_aisles = [i for i in list_features if i not in list_mls]\n",
    "features_fail_mls = features_aisles[:,list_mls]\n",
    "features_fail_conv_aisles = features_aisles[:,list_conv_aisles]\n",
    "\n",
    "sums_conv_aisles = []\n",
    "for i in range(0, features_fail_conv_aisles.shape[1], 2):\n",
    "    if i + 1 < features_fail_conv_aisles.shape[1]:\n",
    "        sums_conv_aisles.append(np.sum(features_fail_conv_aisles[:, i:i + 2], axis=1))\n",
    "\n",
    "sums_conv_aisles = np.array(sums_conv_aisles)\n",
    "sums_conv_aisles_inverted = np.transpose(sums_conv_aisles)\n",
    "\n",
    "### picking stations\n",
    "\n",
    "features_fail_conv_pick = new_state[:,-8:]\n",
    "\n",
    "sums_pick = []\n",
    "for i in range(0, features_fail_conv_pick.shape[1], 2):\n",
    "    if i + 1 < features_fail_conv_pick.shape[1]:\n",
    "        sums_pick.append(np.sum(features_fail_conv_pick[:, i:i + 2], axis=1))\n",
    "\n",
    "sums_pick = np.array(sums_pick)\n",
    "sums_pick_inverted = np.transpose(sums_pick)\n",
    "\n",
    "### conveyor loop\n",
    "\n",
    "list_features = list(range(20,50))\n",
    "features_conv_loop = new_state[:,list_features]\n",
    "\n",
    "list_features = list(range(30))\n",
    "list_to_remove = [0,2,12,14,18]\n",
    "list_conv_loop = [i for i in list_features if i not in list_to_remove]\n",
    "features_fail_conv_loop = features_conv_loop[:,list_conv_loop]\n",
    "\n",
    "features_fail_conv_loop[:, 11] += features_fail_conv_loop[:, 12]\n",
    "features_fail_conv_loop = np.delete(features_fail_conv_loop, 12, axis=1)\n",
    "\n",
    "features_fail_conv_loop[:, 22] += features_fail_conv_loop[:, 23]\n",
    "features_fail_conv_loop = np.delete(features_fail_conv_loop, 23, axis=1)\n",
    "\n",
    "features_new_state = np.concatenate((features_fail_mls,sums_conv_aisles_inverted,features_fail_conv_loop,sums_pick_inverted),axis=1)\n",
    "print(features_new_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:44:18.510018Z",
     "start_time": "2025-05-26T09:44:18.502903Z"
    }
   },
   "outputs": [],
   "source": [
    "# concat linear features\n",
    "\n",
    "#features_rec = features_order_sets_all\n",
    "features_rec = slided_features_order_sets_all\n",
    "#features_lin = np.concatenate((features_racks_all,features_upstate_all,features_failures_all),axis=1)\n",
    "features_lin = np.concatenate((slided_features_racks_all,features_new_state),axis=1)\n",
    "\n",
    "print(features_rec.shape)\n",
    "print(features_lin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:44:31.568794Z",
     "start_time": "2025-05-26T09:44:31.060683Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Calculate correlation matrix using NumPy\n",
    "correlation_matrix = np.corrcoef(features_lin, rowvar=False)\n",
    "correlation_matrix, _ = spearmanr(features_lin, axis=0)\n",
    "\n",
    "# Step 3: Plot the correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(correlation_matrix, cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label='Correlation Coefficient')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3500 è l'indice delle simulazioni senza forzare ordini grandi\n",
    "threeshold = 3500\n",
    "\n",
    "features_rec = features_rec[:threeshold,:,:]\n",
    "features_lin = features_lin[:threeshold,:]\n",
    "output_ann = output_ann_all[:threeshold,:]\n",
    "\n",
    "print(features_rec.shape)\n",
    "print(features_lin.shape)\n",
    "print(output_ann.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_hist(data):\n",
    "    # Plot the histogram\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.hist(data, bins=30, color='blue', alpha=0.7)\n",
    "    plt.title('Histogram of NumPy Array')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(output_ann[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = output_ann_all[:,0]\n",
    "\n",
    "Q1 = np.percentile(data_all, 25)\n",
    "Q3 = np.percentile(data_all, 75)\n",
    "IQR = Q3 - Q1\n",
    "cut_off = 1.5\n",
    "outliers = np.where((data_all < Q1 - cut_off * IQR) | (data_all > Q3 + cut_off * IQR))\n",
    "\n",
    "print(f\"Number of outliers -> {len(outliers[0])}\")\n",
    "print(f\"Values of outliers: -> {data_all[outliers]}\")\n",
    "\n",
    "features_rec  = np.delete(features_rec, outliers, axis=0)\n",
    "features_lin  = np.delete(features_lin, outliers, axis=0)\n",
    "output_ann = np.delete(output_ann, outliers, axis=0)\n",
    "\n",
    "print(features_rec.shape)\n",
    "print(features_lin.shape)\n",
    "print(output_ann.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(output_ann[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "indices = np.where(output_ann[:,0] > x)\n",
    "\n",
    "sliced_arr = features_rec[indices]\n",
    "#print(\"Sliced array based on indices:\", sliced_arr[0,:])\n",
    "\n",
    "# Function to count rows with all values equal to 0 in a batch\n",
    "def count_zero_rows(batch):\n",
    "    return np.sum(np.all(batch == 0, axis=1))\n",
    "\n",
    "columns_to_sum = [0,1,2]\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over batches\n",
    "for batch_index in range(sliced_arr.shape[0]):\n",
    "    res = []\n",
    "    batch = sliced_arr[batch_index]\n",
    "    batch_size = batch.shape[0]\n",
    "    \n",
    "    # Count rows with all values equal to 0 in the batch\n",
    "    zero_rows_count = count_zero_rows(batch)\n",
    "    \n",
    "    # Calculate the length of the batch minus the number of zero rows\n",
    "    effective_length = batch_size - zero_rows_count\n",
    "    res.append(effective_length)\n",
    "\n",
    "    # Calculate the sum along the specified columns\n",
    "    res.append(np.mean(batch[:, columns_to_sum[0]]))\n",
    "    res.append(np.mean(batch[:, columns_to_sum[1]]))\n",
    "    res.append(np.mean(batch[:, columns_to_sum[2]]))\n",
    "\n",
    "    res.append(output_ann[batch_index,0])\n",
    "\n",
    "    results.append(res)\n",
    "\n",
    "df = pd.DataFrame(results, columns=['n order', 'n prod a', 'n prod b', 'n prod c','time'])\n",
    "\n",
    "# Plot histograms for each column with a bigger plot\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "df['n order'].hist(ax=axes[0, 0], bins=10)\n",
    "df['n prod a'].hist(ax=axes[0, 1], bins=10)\n",
    "df['n prod b'].hist(ax=axes[1, 0], bins=10)\n",
    "df['n prod c'].hist(ax=axes[1, 1], bins=10)\n",
    "\n",
    "axes[0, 0].set_title('n_order')\n",
    "axes[0, 1].set_title('n prod a')\n",
    "axes[1, 0].set_title('n prod b')\n",
    "axes[1, 1].set_title('n prod c')\n",
    "\n",
    "fig.suptitle('Histograms of Columns')\n",
    "plt.show()\n",
    "\n",
    "# Plot scatter plots against 'time' for each column with a bigger plot\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "df.plot.scatter(x='n order', y='time', ax=axes[0, 0], s=10)\n",
    "df.plot.scatter(x='n prod a', y='time', ax=axes[0, 1], s=10)\n",
    "df.plot.scatter(x='n prod b', y='time', ax=axes[1, 0], s=10)\n",
    "df.plot.scatter(x='n prod c', y='time', ax=axes[1, 1], s=10)\n",
    "\n",
    "fig.suptitle('Scatter Plots against Time (Inverted Axes)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = features_rec\n",
    "X2 = features_lin\n",
    "Y = output_ann\n",
    "\n",
    "if phase == \"metamodel_training_and_testing\":\n",
    "    \n",
    "    directory = os.path.join(current_directory,\"1_data\",\"processed\")\n",
    "    indexes = np.arange(output_ann.shape[0])\n",
    "    size_split = int(np.ceil(0.8*len(indexes)))\n",
    "\n",
    "    np.random.shuffle(indexes)\n",
    "    training_indexes = np.random.choice(indexes, size=size_split, replace=False)\n",
    "    testing_indexes = [x for x in indexes if x not in training_indexes]\n",
    "\n",
    "    dict_indexes = {\n",
    "        'training' : training_indexes,\n",
    "        'testing' : testing_indexes\n",
    "    }\n",
    "\n",
    "    processes = ['training','testing']\n",
    "\n",
    "    folders = {\n",
    "        'training' : directory + os.sep +'metamodel_training',\n",
    "        'testing' : directory + os.sep +'metamodel_testing'\n",
    "    }\n",
    "        \n",
    "    # Salva i dati split\n",
    "    for split in ['training', 'testing']:\n",
    "        idx = dict_indexes[split]\n",
    "        np.save(os.path.join(folders[split], \"features_rec.npy\"), X1[idx])\n",
    "        np.save(os.path.join(folders[split], \"features_lin.npy\"), X2[idx])\n",
    "        np.save(os.path.join(folders[split], \"output.npy\"), Y[idx])\n",
    "\n",
    "else:\n",
    "    #folder = folders[process]\n",
    "    np.save(os.path.join(directory,\"features_rec.npy\"),X1)\n",
    "    np.save(os.path.join(directory,\"features_lin.npy\"),X2)\n",
    "    np.save(os.path.join(directory,\"output.npy\"),Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metasimopt-py3.10 (3.10.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
